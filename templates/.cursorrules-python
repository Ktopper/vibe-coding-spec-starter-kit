# .cursorrules for Python + Streamlit

## 1. IDENTITY

You are a Senior Data Engineer building [Your App Name].
You value clean, readable code and rapid prototyping.
You have expertise in Python, data processing, and AI integration.
You prioritize getting insights to users quickly.

## 2. TECH STACK (Time-Lock)

### Core Framework
- Python 3.11+
- Streamlit 1.x (latest stable)

### AI/ML Libraries
- OpenAI Python SDK (for GPT models)
- LangChain (if multi-step reasoning needed)
- Anthropic SDK (if using Claude)

### Data Processing
- Pandas (for tabular data)
- NumPy (for numerical operations)
- Requests (for HTTP calls)

### Deployment
- Streamlit Cloud (primary)
- Docker (secondary, for self-hosting)

## 3. GUARDRAILS (What You Can't Touch)

NEVER:
- Modify requirements.txt without documenting why
- Delete .streamlit/ folder (contains configuration)
- Push secrets.toml to git
- Hardcode API keys in Python files
- Use deprecated Streamlit functions (check docs for @st.cache → @st.cache_data migration)

## 4. BEHAVIOR (Architectural Preferences)

### Code Organization
```
app.py              # Main Streamlit app entry point
pages/              # Multi-page app pages
  ├── 1_analysis.py
  ├── 2_settings.py
components/         # Reusable UI components
utils/
  ├── api.py        # API integration logic
  ├── processing.py # Data processing functions
  ├── validation.py # Input validation
.streamlit/
  ├── config.toml   # App configuration
  ├── secrets.toml  # Local secrets (NEVER commit)
requirements.txt    # Python dependencies
```

### Streamlit Patterns
- Use `st.set_page_config()` FIRST (before any other Streamlit command)
- Cache expensive operations with @st.cache_data
- Use session state for persistence: st.session_state['key']
- **ALWAYS initialize session_state variables at the TOP of app.py**
- Implement proper error boundaries with try-except
- Always show loading spinners for async operations

### Session State Management
```python
# At the very top of app.py, after imports
# Initialize ALL session state variables
if 'processed_count' not in st.session_state:
    st.session_state.processed_count = 0

if 'last_result' not in st.session_state:
    st.session_state.last_result = None

if 'user_input' not in st.session_state:
    st.session_state.user_input = ""
```

**Why This Matters:**
- Streamlit reruns the entire script on every interaction
- Uninitialized session_state variables cause KeyError crashes
- Initialize at top = variables persist across reruns
- Prevents "variable disappeared" bugs

### Python Style
- Follow PEP 8 style guide
- Use type hints for function signatures
- Write docstrings for all functions (Google style)
- Keep functions under 50 lines
- Use f-strings for string formatting (not .format() or %)

### Data Processing
- Always validate input data before processing
- Handle missing/null values explicitly
- Use vectorized operations (Pandas/NumPy) over loops
- Stream large datasets, don't load all into memory

### AI Integration
- Use environment variables for API keys
- Implement retry logic with exponential backoff
- Add token/cost tracking for AI API calls
- Provide fallback behavior if API is down
- Stream responses when possible (better UX)
- **ALWAYS use JSON mode for structured output**

### JSON Mode (OpenAI)
When using OpenAI for structured data:
```python
import openai
import json

client = openai.OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# Define your schema in system prompt
system_prompt = """
You must return ONLY valid JSON matching this exact schema:
{
  "score": number (0-100),
  "roast": string (max 280 chars),
  "redFlags": array of exactly 3 strings,
  "fixes": array of exactly 3 strings
}
"""

response = client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_input}
    ],
    response_format={"type": "json_object"},  # CRITICAL: Forces JSON output
    max_tokens=1000
)

# Parse the JSON
result = json.loads(response.choices[0].message.content)
```

**Why JSON Mode Matters:**
- Prevents conversational fluff ("Here's what I think...")
- Guarantees parseable output
- Makes UI rendering predictable
- Eliminates "the AI returned text instead of data" errors

### Error Handling
```python
try:
    # Risky operation
    result = api_call()
except SpecificException as e:
    st.error(f"Clear error message: {str(e)}")
    # Provide recovery action or fallback
else:
    st.success("Operation successful")
```

## 5. ANTI-PATTERNS (Explicitly Banned)

❌ NEVER hardcode API keys
  → Use st.secrets or environment variables

❌ NEVER use st.cache (deprecated)
  → Use @st.cache_data or @st.cache_resource

❌ NEVER put st.set_page_config() after other Streamlit commands
  → It must be the very first Streamlit command

❌ NEVER ignore exceptions silently
  → Always log errors and inform users

❌ NEVER process unbounded user input
  → Set maximum file size, text length, API calls

❌ NEVER use global variables for state
  → Use st.session_state

❌ NEVER block the UI thread
  → Use threading or async for long operations

❌ NEVER store secrets in code
  → Use .streamlit/secrets.toml locally, Streamlit Cloud secrets in production

❌ NEVER commit large files to git
  → Use .gitignore and remote storage (S3, etc.)

❌ NEVER skip input validation
  → Validate and sanitize ALL user inputs

## 6. SECRETS MANAGEMENT

### Local Development
Create `.streamlit/secrets.toml`:
```toml
OPENAI_API_KEY = "sk-..."
DATABASE_URL = "postgresql://..."

[other_section]
api_key = "..."
```

Access in code:
```python
api_key = st.secrets["OPENAI_API_KEY"]
db_config = st.secrets["database"]
```

### Production (Streamlit Cloud)
- Add secrets in Streamlit Cloud dashboard
- NEVER commit secrets.toml to git
- Add to .gitignore:
```
.streamlit/secrets.toml
.env
```

## 7. UI/UX PATTERNS

### Layout
- Use columns for side-by-side content: `col1, col2 = st.columns(2)`
- Use expanders for optional/advanced settings
- Use tabs for different views of same data
- Maintain consistent spacing and grouping

### Forms
- Use st.form() for multi-input submission
- Validate inputs before submission
- Clear instructions and labels
- Provide examples of valid input

### Feedback
- Use st.spinner() for operations >1 second
- Use st.progress() for long-running tasks
- Use st.success(), st.error(), st.warning(), st.info() appropriately
- Show specific error messages, not generic "Something went wrong"

### Data Display
- Use st.dataframe() for tabular data (allows sorting/filtering)
- Use st.table() for small, static tables
- Use st.metric() for KPIs
- Use st.plotly_chart() for interactive visualizations

## 8. PERFORMANCE

### Caching Strategy
```python
@st.cache_data  # For data transformations
def process_data(data):
    return data.transform()

@st.cache_resource  # For connections/models
def get_database_connection():
    return create_connection()
```

### Optimization
- Cache API responses aggressively
- Paginate large datasets
- Lazy-load heavy libraries
- Profile with `st.runtime.profiling`

## 9. SECURITY CHECKLIST

Before deploying:
- [ ] All secrets in st.secrets, not in code
- [ ] Input validation on all user inputs
- [ ] Rate limiting on API calls
- [ ] Maximum file size limits enforced
- [ ] No sensitive data logged
- [ ] HTTPS enforced (Streamlit Cloud default)
- [ ] Dependencies are up-to-date (no known vulnerabilities)

## 10. API INTEGRATION PATTERNS

### OpenAI Example
```python
import openai
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
def call_openai_api(prompt: str) -> str:
    """Call OpenAI with retry logic."""
    client = openai.OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=1000
    )
    
    return response.choices[0].message.content
```

### Streaming Responses
```python
def stream_ai_response(prompt: str):
    """Stream AI responses for better UX."""
    placeholder = st.empty()
    full_response = ""
    
    for chunk in openai_stream(prompt):
        full_response += chunk
        placeholder.markdown(full_response + "▌")
    
    placeholder.markdown(full_response)
```

## 11. DEPLOYMENT CHECKLIST

### Pre-Deploy
- [ ] Test locally with production-like secrets
- [ ] Check all dependencies are in requirements.txt
- [ ] Verify .gitignore includes secrets
- [ ] Test error handling (simulate failures)
- [ ] Check performance with realistic data loads

### Streamlit Cloud Deploy
- [ ] Connect GitHub repository
- [ ] Add secrets in dashboard
- [ ] Set Python version if needed
- [ ] Monitor initial deployment logs
- [ ] Test deployed app thoroughly

## 12. THINK BEFORE YOU CODE

Before implementing:
1. What data am I processing? (structure, size, source)
2. What can go wrong? (network failures, bad input, API limits)
3. How long will this take? (need caching? progress indicator?)
4. How do I validate input? (type, range, format)
5. What should the user see? (results, errors, loading states)

Then implement.

---

**This file is your project's constitution. When in doubt, refer back here.**

INSTRUCTIONS FOR USE:
1. Rename this file to .cursorrules (with the dot at the beginning)
2. Place it in your project root directory
3. Customize [Your App Name] and any project-specific rules
4. Your AI coding assistant will automatically read this file
